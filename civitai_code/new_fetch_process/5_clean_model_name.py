import json
import csv
import re
import time
import requests

# ================= é…ç½®åŒºåŸŸ =================
INPUT_FILE = r"F:\civitai_new_fetch\summary\merged_only_images.jsonl"          # è¾“å…¥æ–‡ä»¶
OUTPUT_CSV = r"F:\civitai_new_fetch\summary\final_clean_model_checkpoint_dataset.csv"  
OUTPUT_JSONL = r"F:\civitai_new_fetch\summary\final_clean_model_checkpoint_dataset.jsonl"
API_DELAY = 0.5                                  # API å»¶è¿Ÿ
# ===========================================

# ç¼“å­˜ä¸æ­£åˆ™å®šä¹‰
model_cache = {}
uuid_pattern = re.compile(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', re.IGNORECASE)
id_filename_pattern = re.compile(r'^(\d+)(\.safetensors|\.ckpt|\.pt)?$', re.IGNORECASE)
prefix_pattern = re.compile(r'^\d+[_ \.-]?')
suffix_pattern = re.compile(r'[_ -]?(fp8|fp16|bf16|nf4|int8|noclip|gguf|q4_k|q8_0|pruned|baked|vae).*', re.IGNORECASE)
ext_pattern = re.compile(r'\.(safetensors|ckpt|pt)$', re.IGNORECASE)

def fetch_model_info_from_api(version_id=None, file_hash=None):
    """API è”ç½‘æŸ¥è¯¢"""
    cache_key = str(version_id) if version_id else f"hash_{file_hash}"
    if cache_key in model_cache: return model_cache[cache_key]

    url = ""
    if version_id: url = f"https://civitai.com/api/v1/model-versions/{version_id}"
    elif file_hash: url = f"https://civitai.com/api/v1/model-versions/by-hash/{file_hash}"
    else: return None

    try:
        print(f"   ğŸŒ [API] æ­£åœ¨æŸ¥è¯¢: {cache_key} ...")
        response = requests.get(url, timeout=5)
        if response.status_code == 200:
            data = response.json()
            name = f"{data.get('model', {}).get('name', 'Unknown')} {data.get('name', '')}"
            model_cache[cache_key] = name
            time.sleep(API_DELAY)
            return name
    except:
        pass
    
    model_cache[cache_key] = "API_Fail"
    return None

def clean_technical_name(raw_name):
    """æ¸…æ´—æŠ€æœ¯æ–‡ä»¶å"""
    if not raw_name: return "Unknown"
    name = str(raw_name).strip()
    name = ext_pattern.sub('', name)
    name = prefix_pattern.sub('', name)
    name = suffix_pattern.sub('', name)
    return name.strip('_ -')

def process_strict_clean_export_all():
    print(f"ğŸš€ å¼€å§‹æ‰§è¡Œï¼šä¸¥æ ¼ç­›é€‰ + æ™ºèƒ½æ¸…æ´— + åŒé‡å¯¼å‡º (CSV & JSONL)...")
    
    csv_headers = ['id', 'baseModel', 'clean_merged_name', 'original_name', 'data_source', 'url']
    
    valid_count = 0
    skipped_not_checkpoint = 0
    skipped_no_name = 0
    
    # åŒæ—¶æ‰“å¼€ CSV å’Œ JSONL æ–‡ä»¶è¿›è¡Œå†™å…¥
    with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8-sig') as f_csv, \
         open(OUTPUT_JSONL, 'w', encoding='utf-8') as f_jsonl, \
         open(INPUT_FILE, 'r', encoding='utf-8') as infile:
        
        writer = csv.writer(f_csv)
        writer.writerow(csv_headers)
        
        for line in infile:
            line = line.strip()
            if not line: continue
            
            try:
                data = json.loads(line)
                
                # --- å…³å¡ 1: å¿…é¡»æ˜¯å›¾ç‰‡ ---
                if data.get('type') != 'image': continue
                
                # --- å…³å¡ 2: å¿…é¡»æœ‰ BaseModel ---
                base_model = data.get('baseModel')
                if not base_model: continue
                
                # --- æå–é˜¶æ®µ ---
                local_name = None
                local_hash = None
                local_id = None
                source_type = "None"
                found_checkpoint_evidence = False

                meta = data.get('meta', {})
                if isinstance(meta, dict) and 'meta' in meta: meta = meta['meta']
                if not isinstance(meta, dict): meta = {}

                # A. æ£€æŸ¥ resources
                res_list = meta.get('resources', [])
                if isinstance(res_list, list):
                    for r in res_list:
                        if r.get('type') == 'model':
                            local_name = r.get('name')
                            local_hash = r.get('hash')
                            source_type = "resources"
                            found_checkpoint_evidence = True
                            break
                
                # B. æ£€æŸ¥ civitaiResources
                civ_list = meta.get('civitaiResources', [])
                if isinstance(civ_list, list):
                    for r in civ_list:
                        if r.get('type') == 'checkpoint':
                            found_checkpoint_evidence = True
                            if not local_id: local_id = r.get('modelVersionId')
                            if not local_name: source_type = "civitaiResources"
                            break

                # C. æ£€æŸ¥ meta.Model
                if not found_checkpoint_evidence:
                    m = meta.get('Model')
                    if m:
                        found_checkpoint_evidence = True
                        if not str(m).startswith('urn:'):
                            local_name = m
                            source_type = "meta.Model"
                
                # --- å…³å¡ 3: ä¸¥æ ¼ Checkpoint æ ¡éªŒ ---
                if not found_checkpoint_evidence:
                    skipped_not_checkpoint += 1
                    continue

                # --- æ¸…æ´—ä¸ä¿®å¤é˜¶æ®µ ---
                final_name = local_name
                is_bad = False
                
                if not local_name: is_bad = True
                elif uuid_pattern.match(str(local_name)) or id_filename_pattern.match(str(local_name)):
                    is_bad = True
                
                if is_bad and (local_id or local_hash):
                    api_name = fetch_model_info_from_api(local_id, local_hash)
                    if api_name and "API_Fail" not in api_name:
                        final_name = api_name
                        source_type = "API_Fixed"
                    else:
                        final_name = f"Unknown_Hash_{local_hash}" if local_hash else "Unknown_UUID"

                if final_name is None:
                    final_name = "Unknown"

                clean_name = final_name
                if source_type != "API_Fixed" and "Unknown" not in str(final_name):
                    clean_name = clean_technical_name(final_name)
                
                # --- å†æ¬¡ç¡®è®¤æœ‰æ•ˆæ€§ ---
                if "Unknown" in str(clean_name) and not local_hash and not local_id:
                    skipped_no_name += 1
                    continue

                # ================= æ ¸å¿ƒä¿®æ”¹ï¼šæ›´æ–°å¹¶å†™å…¥æ•°æ® =================
                
                # 1. å†™å…¥ CSV (ä»…æ‘˜è¦)
                writer.writerow([
                    data.get('id'),
                    base_model,
                    clean_name,
                    local_name,
                    source_type,
                    data.get('url')
                ])
                
                # 2. å†™å…¥ JSONL (å®Œæ•´æ•°æ® + æ–°å¢å­—æ®µ)
                # æˆ‘ä»¬æŠŠæ¸…æ´—å‡ºæ¥çš„å…³é”®ä¿¡æ¯æ³¨å…¥åˆ° JSON å¯¹è±¡é‡Œï¼Œæ–¹ä¾¿ä»¥åä½¿ç”¨
                data['clean_merged_name'] = clean_name       # æœ€é‡è¦çš„æ¸…æ´—å
                data['original_model_name'] = local_name     # åŸå§‹å
                data['model_source_type'] = source_type      # æ¥æº
                
                # å¦‚æœæ˜¯ API ä¿®å¤çš„ï¼Œè¿˜å¯ä»¥æŠŠ ID è¡¥è¿›å»
                if local_id:
                    data['fixed_model_id'] = local_id

                f_jsonl.write(json.dumps(data, ensure_ascii=False) + '\n')
                
                valid_count += 1
                
            except json.JSONDecodeError:
                continue

    print(f"\nâœ… å…¨éƒ¨å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“¥ æœ‰æ•ˆæ•°æ®: {valid_count} æ¡")
    print(f"ğŸš« å‰”é™¤æ— Checkpoint: {skipped_not_checkpoint} æ¡")
    print(f"ğŸ—‘ï¸ å‰”é™¤æ— åæ•°æ®: {skipped_no_name} æ¡")
    print(f"------------------------------------------------")
    print(f"ğŸ“Š CSV æŠ¥è¡¨: {OUTPUT_CSV}")
    print(f"ğŸ’¾ JSONL æ•°æ®: {OUTPUT_JSONL} (å·²åŒ…å« clean_merged_name å­—æ®µ)")

if __name__ == "__main__":
    process_strict_clean_export_all()
# import json
# import csv
# import re
# import time
# import requests

# # ================= é…ç½®åŒºåŸŸ =================
# INPUT_FILE = r"F:\civitai_new_fetch\summary\merged_only_images.jsonl"          # è¾“å…¥æ–‡ä»¶
# OUTPUT_CSV = r"F:\civitai_new_fetch\summary\final_clean_dataset.csv"           # è¾“å‡ºæ–‡ä»¶
# API_DELAY = 0.5                                  # API è¯·æ±‚é—´éš”
# # ===========================================

# # ç¼“å­˜å­—å…¸
# model_cache = {}

# # æ­£åˆ™å®šä¹‰
# # 1. ååå­—ï¼šUUID æˆ– çº¯æ•°å­—ID
# uuid_pattern = re.compile(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', re.IGNORECASE)
# id_filename_pattern = re.compile(r'^(\d+)(\.safetensors|\.ckpt|\.pt)?$', re.IGNORECASE)

# # 2. æ¸…æ´—è§„åˆ™ï¼šå»é™¤å‰ç¼€æ•°å­— (å¦‚ "2758_") å’Œ æŠ€æœ¯åç¼€ (å¦‚ "fp8", "noclip")
# # åŒ¹é…å¼€å¤´çš„æ•°å­—+éå•è¯å­—ç¬¦ (å¦‚ "1234_", "01 ")
# prefix_pattern = re.compile(r'^\d+[_ \.-]?')
# # åŒ¹é…å¸¸è§çš„é‡åŒ–/ä¿®å‰ªåç¼€ (ä¸åŒºåˆ†å¤§å°å†™)
# suffix_pattern = re.compile(r'[_ -]?(fp8|fp16|bf16|nf4|int8|noclip|gguf|q4_k|q8_0|pruned|baked|vae).*', re.IGNORECASE)
# # åŒ¹é…æ–‡ä»¶æ‰©å±•å
# ext_pattern = re.compile(r'\.(safetensors|ckpt|pt)$', re.IGNORECASE)

# def fetch_model_info_from_api(version_id=None, file_hash=None):
#     """API è”ç½‘æŸ¥è¯¢"""
#     cache_key = str(version_id) if version_id else f"hash_{file_hash}"
#     if cache_key in model_cache: return model_cache[cache_key]

#     url = ""
#     if version_id: url = f"https://civitai.com/api/v1/model-versions/{version_id}"
#     elif file_hash: url = f"https://civitai.com/api/v1/model-versions/by-hash/{file_hash}"
#     else: return None

#     try:
#         print(f"   ğŸŒ [API] æ­£åœ¨æŸ¥è¯¢: {cache_key} ...")
#         response = requests.get(url, timeout=5)
#         if response.status_code == 200:
#             data = response.json()
#             # ç»„åˆåç§°ï¼šæ¨¡å‹å + ç‰ˆæœ¬å
#             name = f"{data.get('model', {}).get('name', 'Unknown')} {data.get('name', '')}"
#             model_cache[cache_key] = name
#             time.sleep(API_DELAY)
#             return name
#     except:
#         pass
    
#     model_cache[cache_key] = "API_Fail"
#     return None

# def clean_technical_name(raw_name):
#     """
#     æ¸…æ´—æŠ€æœ¯æ–‡ä»¶åï¼Œç”¨äºåˆå¹¶ç»Ÿè®¡
#     è¾“å…¥: 2758FluxAsianUtopian_v51KreaFp8Noclip.safetensors
#     è¾“å‡º: FluxAsianUtopian_v51Krea
#     """
#     if not raw_name: return "Unknown"
    
#     name = str(raw_name).strip()
    
#     # 1. å»æ‰æ‰©å±•å
#     name = ext_pattern.sub('', name)
    
#     # 2. å»æ‰å¼€å¤´çš„æ’åºæ•°å­— (å¦‚ "2758")
#     name = prefix_pattern.sub('', name)
    
#     # 3. å»æ‰æŠ€æœ¯åç¼€ (å¦‚ "Fp8", "Noclip")
#     name = suffix_pattern.sub('', name)
    
#     # 4. å»æ‰å¤šä½™çš„ä¸‹åˆ’çº¿æˆ–ç©ºæ ¼
#     name = name.strip('_ -')
    
#     return name

# def process_and_clean():
#     print(f"ğŸš€ å¼€å§‹å¤„ç†æ•°æ®ï¼Œç”Ÿæˆæ¸…æ´—åçš„åˆå¹¶åˆ—...")
    
#     # æ–°å¢åˆ—ï¼šclean_merged_name
#     headers = ['id', 'baseModel', 'clean_merged_name', 'original_name', 'data_source', 'url']
    
#     valid_count = 0
    
#     with open(OUTPUT_CSV, 'w', newline='', encoding='utf-8-sig') as f_csv, \
#          open(INPUT_FILE, 'r', encoding='utf-8') as infile:
        
#         writer = csv.writer(f_csv)
#         writer.writerow(headers)
        
#         for line in infile:
#             line = line.strip()
#             if not line: continue
            
#             try:
#                 data = json.loads(line)
#                 if data.get('type') != 'image': continue
                
#                 base_model = data.get('baseModel')
#                 if not base_model: continue
                
#                 # --- 1. æå–åŸå§‹ä¿¡æ¯ ---
#                 local_name = None
#                 local_hash = None
#                 local_id = None
#                 source_type = "None"
                
#                 # å¤„ç†åµŒå¥— meta
#                 meta = data.get('meta', {})
#                 if isinstance(meta, dict) and 'meta' in meta: meta = meta['meta']
#                 if not isinstance(meta, dict): meta = {}

#                 # ä¼˜å…ˆæ‰¾ resources
#                 res_list = meta.get('resources', [])
#                 if isinstance(res_list, list):
#                     for r in res_list:
#                         if r.get('type') == 'model':
#                             local_name = r.get('name')
#                             local_hash = r.get('hash')
#                             source_type = "resources"
#                             break
                
#                 # è¡¥æ¼æ‰¾ civitaiResources (ä¸ºäº†ID)
#                 civ_list = meta.get('civitaiResources', [])
#                 if isinstance(civ_list, list):
#                     for r in civ_list:
#                         if r.get('type') == 'checkpoint':
#                             if not local_id: local_id = r.get('modelVersionId')
#                             if not local_name: source_type = "civitaiResources" # æ­¤æ—¶è¿˜æ²¡åå­—
#                             break

#                 # å…œåº•æ‰¾ meta.Model
#                 if not local_name:
#                     m = meta.get('Model')
#                     if m and not str(m).startswith('urn:'):
#                         local_name = m
#                         source_type = "meta.Model"

#                 # --- 2. å†³ç­–ä¸ä¿®å¤ ---
#                 final_name = local_name
#                 is_bad = False
                
#                 # åˆ¤å®šæ˜¯å¦ä¸ºâ€œååå­—â€ (UUID / çº¯æ•°å­— / ç©º)
#                 if not local_name: is_bad = True
#                 elif uuid_pattern.match(str(local_name)) or id_filename_pattern.match(str(local_name)):
#                     is_bad = True
                
#                 # å¦‚æœæ˜¯ååå­—ï¼Œå°è¯• API ä¿®å¤
#                 if is_bad and (local_id or local_hash):
#                     api_name = fetch_model_info_from_api(local_id, local_hash)
#                     if api_name and "API_Fail" not in api_name:
#                         final_name = api_name
#                         source_type = "API_Fixed"
#                     else:
#                         # API ä¹Ÿæ²¡æ•‘å›æ¥ï¼Œåªèƒ½ç”¨ Hash ä»£æ›¿ï¼Œä¿è¯è¿™ä¸€åˆ—æœ‰å€¼
#                         final_name = f"Unknown_Hash_{local_hash}" if local_hash else "Unknown"

#                 # --- 3. æœ€ç»ˆæ¸…æ´— (ç”Ÿæˆåˆå¹¶åˆ—) ---
#                 # å¦‚æœæ˜¯ API ä¿®å¤å›æ¥çš„åå­— (å¦‚ "Chroma v5")ï¼Œé€šå¸¸å¾ˆå¹²å‡€ï¼Œä¸éœ€è¦æ­£åˆ™æ´—
#                 # å¦‚æœæ˜¯æœ¬åœ°æ–‡ä»¶å (å¦‚ "2758Flux...Fp8")ï¼Œéœ€è¦æ­£åˆ™æ´—
                
#                 clean_name = final_name
#                 # if source_type != "API_Fixed" and "Unknown" not in final_name:
#                 # # å¼ºåˆ¶æŠŠ final_name è½¬æˆå­—ç¬¦ä¸²å†åˆ¤æ–­ï¼Œè¿™æ · None å°±ä¼šå˜æˆ "None"ï¼Œå°±ä¸ä¼šæŠ¥é”™äº†
#                 if source_type != "API_Fixed" and "Unknown" not in str(final_name):
#                     clean_name = clean_technical_name(final_name)
                
#                 # --- 4. å†™å…¥ ---
#                 writer.writerow([
#                     data.get('id'),
#                     base_model,
#                     clean_name,      # <--- è¿™æ˜¯ä½ è¦çš„åˆå¹¶åˆ—
#                     local_name,      # åŸå§‹åå­— (æ–¹ä¾¿æŸ¥è¯)
#                     source_type,
#                     data.get('url')
#                 ])
#                 valid_count += 1
                
#             except json.JSONDecodeError:
#                 continue

#     print(f"\nâœ… å®Œæˆï¼ç»“æœä¿å­˜åœ¨: {OUTPUT_CSV}")
#     print(f"ğŸ“Š è¿™é‡Œçš„ 'clean_merged_name' åˆ—å·²ç»å»é™¤äº† Fp8/Noclip ç­‰åç¼€ï¼Œå¯ç›´æ¥ç”¨äºåˆå¹¶ç»Ÿè®¡ã€‚")

# if __name__ == "__main__":
#     process_and_clean()